const DEBUG = /\b(y|all|tabledata)\b/.test (process.env.DEBUG) && console.warn // eslint-disable-line
const _warn = (message) => { console.error ('[WARNING] [hdbtabledata] -', message) }  // eslint-disable-line
const cds = require ('../cds')
const CSV = require('../utils/csv')
const util = require ('util')
const readdir = util.promisify (cds.utils.readdir)
const readFile = util.promisify (cds.utils.readFile)
const { path, isdir } = cds.utils

module.exports.create = async (model, options={}) => {
  const baseDir = options.baseDir  // where the hdbtabledata will be located, for usage in the file_name path
  const dirs = Array.isArray(options.dirs) ? options.dirs : _csvDirs (model._sources.map (path.dirname))
  if (dirs.length === 0)  return []  // nothing to do

  const datas = (await Promise.all(dirs.map(async dir => {
    let files = []
    if (isdir(dir)) {
      files = await readdir (dir)
    }
    return Promise.all(files.filter (_csvs).map (file => _tabledata4 (dir, file, cds.reflect (model), baseDir)))
  })))
  .reduce((a, b) => a.concat(b), [])
  return _toOutput (datas)
}

async function _tabledata4 (dir, csvFile, model, baseDir)  {
  const baseFileName = path.parse(csvFile).name
  const entityName = baseFileName.replace(/-/g,'.')
  const entity = _entity4 (entityName, model)
  if (!entity) {
    let message = `no entity '${entityName}' found for CSV file '${path.relative(process.cwd(), path.join(dir, csvFile))}'`
    const candidate = Object.keys(model.definitions)
      .filter(name => !name.startsWith('localized.'))
      .find  (name => name.toLowerCase().includes(entityName.toLowerCase()))
    if (candidate)  message += `. Did you mean '${candidate}'?`
    return _warn (message)
  }

  const tabledata = { format_version: 1, imports: [] }
  const _import = {
    target_table: getArtifactDatabaseNameOf (entity.name),
    source_data: { data_type: 'CSV', file_name: csvFile, has_header: true, type_config: {} },
    import_settings: { import_columns: [], include_filter: [] },
    column_mappings: {}
  };

  const file = path.join(dir, csvFile)
  const src = await readFile (file, 'utf8')
  const { cols, delimiter } = CSV.parseHeader (src)
  if (cols.length === 0)  return  // no columns at all -> skip import

  cols.forEach(csvCol => {
    // Only translate the column name, but do not check for the existence of the element in the model.
    // This gets tricky for foreign key elements, and the DB deployment anyways checks the column.
    const tableCol = getArtifactDatabaseNameOf (csvCol)
    _import.import_settings.import_columns.push (tableCol)
    _import.column_mappings[tableCol] = csvCol
  })

  _import.source_data.type_config.delimiter = delimiter

  // add a locale filter for mono-lingual files that refer to generated text tables
  if (entity.elements.locale)  {
    const locale = /_texts_(.+)\.csv$/.test (csvFile) ? RegExp.$1 : null
    if (locale) {
      const localeKey = getArtifactDatabaseNameOf (entity.elements.locale.name) // usually 'LOCALE'
      _import.import_settings.include_filter.push ({ [localeKey]: locale })
    }
  }

  tabledata.imports.push (_import)
  const suffix = '.hdbtabledata'
  return [
    tabledata, {
      name: baseFileName, suffix,
      file: baseFileName + suffix,
      folder: (baseDir||dir), // as metadata, add the dir to which the csvs are relative to
      csvFolder: dir
    }
  ]
}

function _entity4 (name, csn) {
  const entity = csn.definitions [name]
  if (!entity) {
    if (/(.+)_texts_?/.test (name)) { // 'Books_texts', 'Books_texts_de'
      const base = csn.definitions [RegExp.$1]
      if (base && base.elements && base.elements.texts) {
        return _entity4 (base.elements.texts.target, csn)
      }
    }
    return
  }
  if (entity['@cds.persistence.skip'] === true)  return _warn (`exclude skipped entity '${name}'`)
  if (entity.query) {
    let {SELECT} = entity.query
    if (SELECT && SELECT.from.ref && SELECT.from.ref.length === 1) {
      return _entity4 (SELECT.from.ref[0], csn)
    }
  }
  return entity
}

function _csvDirs (sources) {
  sources = Array.from (new Set(sources)) // uniq
  const folders = []
  for (let src of sources) {
    for (let data of ['/data','/csv']) {
      for (let each of [ src+data, src+'/src'+data, src+'/..'+data ]) {
        let folder = path.resolve (each)
        if (isdir (folder))  folders.push (folder)
      }
    }
  }
  return folders
}

function _csvs (filename,_,allFiles) {
  if (filename[0] === '-' || !filename.endsWith ('.csv'))  return false
  // ignores 'Books_texts.csv' if there is any 'Books_texts_LANG.csv'
  if (/(.*)_texts\.csv$/.test (filename)) {
    const basename = RegExp.$1
    const monoLangFiles = allFiles.filter (file => new RegExp(basename+'_texts_').test (file))
    if (monoLangFiles.length > 0) {
      DEBUG && DEBUG (` - ignoring '${filename}' in favor of [${monoLangFiles}]`)  // eslint-disable-line
      return false
    }
  }
  return true
}

// generator function compliant to what `cds.compile.to` backends can return
function* _toOutput (datas) {
  for (let i = 0; i < datas.length; i++) {
    if (datas[i])  yield datas[i]
  }
}

// TODO remove once we have a compiler API
function getArtifactDatabaseNameOf(artifactName, namingConvention=cds.env.sql.names, namespace = undefined) {
  if (namingConvention === 'hdbcds') {
    if (namespace) {
      return `${namespace}::${artifactName.substring(namespace.length + 1)}`;
    }
    return artifactName;
  }
  else if (namingConvention === 'plain') {
    return artifactName.replace(/\./g, '_').toUpperCase();
  }
  else if (namingConvention === 'quoted') {
    return artifactName;
  }
  else {
    throw new Error('Unknown naming convention: ' + namingConvention);
  }
}
